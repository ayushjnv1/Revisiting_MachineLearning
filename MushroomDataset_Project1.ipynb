{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data file using Pandas  \n",
    "mu_data=pd.read_csv('mushrooms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using isnull() function for finding null values   \n",
    "print(mu_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Representation for finding null values using Heat Map\n",
    "sns.heatmap(mu_data.isnull())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu_data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_data1 = mu_data['class'].to_frame()\n",
    "mu_data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the edible=e, poisonous=p \n",
    "c_count=mu_data['class'].value_counts()\n",
    "print(c_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Representation of the dependent variable distribution in the dataset\n",
    "sns.set(rc={'figure.figsize':(7,5)})\n",
    "class_c=mu_data['class']\n",
    "mu_data_count=sns.countplot(x=class_c, data=mu_data)\n",
    "plt.title(\"Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize/Analysis how each independent variables affects the dependent variable\n",
    "def plot_feature(class_target, feature_set):\n",
    "    for k, col in enumerate(mu_data.columns):\n",
    "        plt.figure(k)\n",
    "        sns.set(rc={'figure.figsize': (12,9)})\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        c_count=sns.countplot(x=mu_data[col], hue=class_target, data=mu_data)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all graphs\n",
    "class_c=mu_data['class']\n",
    "feature_to_plot=mu_data.drop('class', axis=1)\n",
    "plot_feature(class_c, feature_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seprate Data into features and target set\n",
    "feature_data=mu_data.drop('class', axis=1)\n",
    "target_class=mu_data['class']\n",
    "print(feature_data.shape)\n",
    "print(target_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Label Encoders for Target and Feaature variables Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder used for the Target class\n",
    "le=LabelEncoder()\n",
    "mu_data['class']= le.fit_transform(mu_data['class'])\n",
    "mu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding applied to feature variables\n",
    "feature_data= pd.get_dummies(feature_data)\n",
    "feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "feature_data_std = sc.fit_transform(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the target variable \n",
    "target_class=mu_data['class'].values.reshape(-1,1)\n",
    "print(target_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will find out random state value with which our linear model learns maximum\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics \n",
    "max_r_score=0\n",
    "for r_state in range(42,200):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(feature_data_std, target_class ,random_state = r_state,test_size=0.30)\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(x_train,y_train)\n",
    "    y_pred = log_reg.predict(x_test)\n",
    "    r2_scr=r2_score(y_test,y_pred)\n",
    "    if r2_scr>max_r_score:\n",
    "        max_r_score=r2_scr\n",
    "        final_r_state=r_state\n",
    "print(\"max r2 score corresponding to \",final_r_state,\" is \",max_r_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate cross validation score to find out the overfitting or underfitting issues while training \n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score=cross_val_score(LogisticRegression(),feature_data_std, target_class,cv=5,scoring=\"r2\")\n",
    "print(\"Cross Validation score for 5 iternations: {}%\".format (cross_val_score.mean()))\n",
    "\n",
    "# As we have achieved significantly impoved score as compared to cross validation. This indicates do not poses any underfitting\n",
    "# and over fitting issues while modelling the ML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalise the model \n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_data_std, target_class, random_state = 42,test_size=0.30)\n",
    "log_reg =LogisticRegression()\n",
    "log_reg.fit(x_train,y_train)\n",
    "y_pred = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Findout the rmse and r2 score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE is: \",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "print(\"r2_score is: \",r2_score(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*Test Accuracy: {}%\".format(round(log_reg.score(x_test,y_test)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#false positive and false negative rates are 0, meaning that all mushrooms were correctly classified as poisonous or not\n",
    "print(\"*Confusion Matrix: \\n {}\".format (confusion_matrix(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n {}\".format (classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classification report visualizer displays the precision, recall, F1, and support scores for the model.\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "mu_viz = ClassificationReport(LogisticRegression(), cmap='GnBu')\n",
    "mu_viz.fit(x_train, y_train)\n",
    "mu_viz.score(x_test, y_test)\n",
    "mu_viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Area under the ROC curve for LogisticRegression\n",
    "Logistic_Reg_ROC_Curve=metrics.roc_auc_score(y_test,y_pred)\n",
    "print(\"ROC Score: \", Logistic_Reg_ROC_Curve)\n",
    "\n",
    "# Define Function to plot the ROC curve \n",
    "def plot_roc_curve(roc_auc):\n",
    "    plt.plot(fp_rate, tp_rate, color='blue', label='ROC = %0.2f' % Logistic_Reg_ROC_Curve)\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='-.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot AUC_ROC curve \n",
    "fp_rate, tp_rate, thresholds= roc_curve(y_test, y_pred)\n",
    "roc_auc=auc(fp_rate, tp_rate)\n",
    "plot_roc_curve(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly Save the model for futher use\n",
    "from sklearn.externals import joblib  \n",
    "joblib.dump(log_reg, 'Mushrooms_poisonous_nonpoisonous.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
